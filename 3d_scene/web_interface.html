<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>3D Multi-Camera Scene Viewer</title>
    <style>
      @import url("https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@300;400;500&family=Space+Grotesk:wght@300;400;500;600&display=swap");

      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      :root {
        --bg-primary: #0a0a0f;
        --bg-secondary: #12121a;
        --bg-tertiary: #1a1a25;
        --accent-cyan: #00d4ff;
        --accent-magenta: #ff00aa;
        --accent-green: #00ff88;
        --text-primary: #ffffff;
        --text-secondary: #8888aa;
        --text-muted: #555566;
        --border-color: #2a2a3a;
      }

      body {
        font-family: "Space Grotesk", sans-serif;
        background: var(--bg-primary);
        color: var(--text-primary);
        min-height: 100vh;
        overflow: hidden;
      }

      #canvas-container {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        z-index: 1;
      }

      canvas {
        display: block;
      }

      /* Header overlay */
      .header {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        padding: 20px 30px;
        background: linear-gradient(
          180deg,
          rgba(10, 10, 15, 0.95) 0%,
          rgba(10, 10, 15, 0) 100%
        );
        z-index: 100;
        display: flex;
        justify-content: space-between;
        align-items: center;
        pointer-events: none;
      }

      .header > * {
        pointer-events: auto;
      }

      .logo {
        display: flex;
        align-items: center;
        gap: 12px;
      }

      .logo-icon {
        width: 36px;
        height: 36px;
        background: #000000;
        border-radius: 8px;
        display: flex;
        align-items: center;
        justify-content: center;
        font-weight: 600;
        font-size: 14px;
      }

      .logo h1 {
        font-size: 1.1em;
        font-weight: 500;
        letter-spacing: -0.5px;
      }

      .logo h1 span {
        color: #ffffff;
      }

      .status-badge {
        display: flex;
        align-items: center;
        gap: 8px;
        background: var(--bg-tertiary);
        padding: 8px 16px;
        border-radius: 20px;
        border: 1px solid var(--border-color);
        font-size: 0.85em;
      }

      .status-dot {
        width: 8px;
        height: 8px;
        border-radius: 50%;
        background: var(--text-muted);
      }

      .status-dot.active {
        background: var(--accent-green);
        box-shadow: 0 0 10px var(--accent-green);
        animation: pulse 2s infinite;
      }

      @keyframes pulse {
        0%,
        100% {
          opacity: 1;
        }
        50% {
          opacity: 0.5;
        }
      }

      .panel-title {
        font-size: 0.75em;
        text-transform: uppercase;
        letter-spacing: 1.5px;
        color: #999999;
        margin-bottom: 15px;
        padding-bottom: 10px;
        border-bottom: 1px solid #333333;
      }

      .control-row {
        display: flex;
        align-items: center;
        justify-content: space-between;
        margin-bottom: 12px;
      }

      .control-row:last-child {
        margin-bottom: 0;
      }

      .control-label {
        font-size: 0.9em;
        color: #999999;
      }

      .control-value {
        font-family: "JetBrains Mono", monospace;
        font-size: 0.85em;
        color: #ffffff;
      }

      /* Camera list panel */
      .camera-panel {
        position: fixed;
        top: 80px;
        left: 20px;
        background: rgba(0, 0, 0, 0.95);
        border: 1px solid #333333;
        border-radius: 8px;
        padding: 20px;
        z-index: 100;
        max-width: 260px;
        max-height: calc(100vh - 120px);
        overflow-y: auto;
        backdrop-filter: blur(20px);
      }

      .camera-item {
        display: flex;
        align-items: center;
        gap: 12px;
        padding: 10px 12px;
        margin-bottom: 8px;
        background: rgba(255, 255, 255, 0.05);
        border-radius: 6px;
        border: 1px solid transparent;
        cursor: pointer;
        transition: all 0.2s ease;
      }

      .camera-item:hover {
        border-color: #555555;
        background: rgba(255, 255, 255, 0.1);
      }

      .camera-item.selected {
        border-color: #ffffff;
        background: rgba(255, 255, 255, 0.15);
      }

      .camera-color {
        width: 12px;
        height: 12px;
        border-radius: 3px;
      }

      .camera-info {
        flex: 1;
      }

      .camera-id {
        font-family: "JetBrains Mono", monospace;
        font-size: 0.85em;
        color: #ffffff;
      }

      .camera-number {
        font-size: 0.75em;
        color: #999999;
      }

      /* Buttons */
      .btn {
        padding: 10px 20px;
        font-size: 0.85em;
        font-weight: 500;
        border: 1px solid var(--border-color);
        border-radius: 6px;
        cursor: pointer;
        transition: all 0.2s ease;
        font-family: "Space Grotesk", sans-serif;
        text-transform: uppercase;
        letter-spacing: 0.5px;
      }

      .btn-primary {
        background: #000000;
        border: none;
        color: white;
      }

      .btn-primary:hover {
        transform: translateY(-1px);
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.5);
        background: #1a1a1a;
      }

      .btn-secondary {
        background: transparent;
        color: var(--text-secondary);
      }

      .btn-secondary:hover {
        background: var(--bg-tertiary);
        color: var(--text-primary);
      }

      .button-group {
        display: flex;
        gap: 10px;
        margin-top: 15px;
      }

      /* Loading overlay */
      .loading-overlay {
        position: fixed;
        top: 0;
        left: 0;
        width: 100%;
        height: 100%;
        background: var(--bg-primary);
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        z-index: 1000;
        transition: opacity 0.5s ease;
      }

      .loading-overlay.hidden {
        opacity: 0;
        pointer-events: none;
      }

      .loader {
        width: 60px;
        height: 60px;
        border: 3px solid var(--bg-tertiary);
        border-top-color: var(--accent-cyan);
        border-radius: 50%;
        animation: spin 1s linear infinite;
      }

      @keyframes spin {
        to {
          transform: rotate(360deg);
        }
      }

      .loading-text {
        margin-top: 20px;
        color: var(--text-secondary);
        font-size: 0.9em;
      }

      /* Instructions */
      .instructions {
        position: fixed;
        bottom: 20px;
        right: 20px;
        background: rgba(18, 18, 26, 0.95);
        border: 1px solid var(--border-color);
        border-radius: 12px;
        padding: 15px 20px;
        z-index: 100;
        backdrop-filter: blur(20px);
      }

      .instruction-item {
        display: flex;
        align-items: center;
        gap: 10px;
        margin-bottom: 8px;
        font-size: 0.8em;
        color: var(--text-secondary);
      }

      .instruction-item:last-child {
        margin-bottom: 0;
      }

      .key {
        background: var(--bg-tertiary);
        padding: 4px 8px;
        border-radius: 4px;
        font-family: "JetBrains Mono", monospace;
        font-size: 0.9em;
        color: var(--text-primary);
        min-width: 60px;
        text-align: center;
      }
    </style>
  </head>
  <body>
    <!-- Loading overlay -->
    <div class="loading-overlay" id="loading">
      <div class="loader"></div>
      <div class="loading-text">Initializing 3D Scene...</div>
    </div>

    <!-- Three.js canvas -->
    <div id="canvas-container"></div>

    <!-- Header -->
    <div class="header">
      <div class="logo">
        <div class="logo-icon">3D</div>
        <h1>Multi-Camera <span>Scene Viewer</span></h1>
      </div>
      <div class="status-badge">
        <div class="status-dot" id="statusDot"></div>
        <span id="statusText">Connecting...</span>
      </div>
    </div>

    <!-- Camera panel with controls -->
    <div class="camera-panel">
      <div class="panel-title">Camera Views</div>
      <div class="control-row">
        <span class="control-label">Cameras</span>
        <span class="control-value" id="cameraCount">0</span>
      </div>
      <div class="button-group" style="margin-bottom: 15px; padding-bottom: 15px; border-bottom: 1px solid #333333;">
        <button class="btn btn-primary" id="startBtn" style="width: 100%;">Start Streams</button>
      </div>
      
      <!-- VGGT Controls -->
      <div class="panel-title" style="margin-top: 15px;">VGGT Point Cloud</div>
      <div class="control-row">
        <label class="control-label" for="vggtEnabled" style="cursor: pointer;">
          <input type="checkbox" id="vggtEnabled" style="margin-right: 8px; cursor: pointer;">
          Enable VGGT
        </label>
        <span class="control-value" id="vggtStatus" style="color: #888;">Off</span>
      </div>
      <div class="control-row">
        <span class="control-label">Interval (frames)</span>
        <select id="vggtInterval" style="background: var(--bg-tertiary); color: var(--text-primary); border: 1px solid var(--border-color); border-radius: 4px; padding: 4px 8px; font-size: 0.85em; cursor: pointer;">
          <option value="6">6</option>
          <option value="10">10</option>
          <option value="20" selected>20</option>
          <option value="30">30</option>
          <option value="60">60</option>
        </select>
      </div>
      <div class="control-row">
        <span class="control-label">Points</span>
        <span class="control-value" id="vggtPoints">0</span>
      </div>
      
      <div style="border-top: 1px solid #333333; margin-top: 15px; padding-top: 15px;">
        <div class="panel-title" style="margin-top: 0;">Cameras</div>
      </div>
      <div id="cameraList"></div>
    </div>

    <!-- Three.js and OrbitControls from CDN -->
    <script type="importmap">
      {
        "imports": {
          "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
          "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/"
        }
      }
    </script>

    <script type="module">
      import * as THREE from "three";
      import { OrbitControls } from "three/addons/controls/OrbitControls.js";
      import { GLTFLoader } from "three/addons/loaders/GLTFLoader.js";
      import { OBJLoader } from "three/addons/loaders/OBJLoader.js";
      import { MTLLoader } from "three/addons/loaders/MTLLoader.js";

      // Color palette for cameras
      const CAMERA_COLORS = [
        0x00d4ff, // cyan
        0xff00aa, // magenta
        0x00ff88, // green
        0xffaa00, // orange
        0xff5555, // red
        0x55ff55, // lime
        0x5555ff, // blue
        0xffff55, // yellow
      ];

      // Scene state
      let scene, camera, renderer, controls;
      let cameraPlanes = {};
      let cameraFrustums = {};
      let calibrationData = {};
      let availableCameras = [];
      let streaming = false;
      let streamIntervals = {};

      // Multi-object model references for real-time pose updates
      let objectModels = {}; // object_name -> THREE.Object3D
      let objectCentroids = {}; // object_name -> THREE.Vector3 (centroid offset)
      let objectConfigs = {}; // object_name -> config from server (includes camera_id)
      let posePollingInterval = null;

      // VGGT point cloud state
      let vggtEnabled = false;
      let vggtPointCloud = null; // THREE.Points object
      let vggtPointCloudGroup = null; // Group to hold point cloud with transformation
      let vggtPollingInterval = null;
      let vggtLastPointCount = 0;
      const VGGT_REFERENCE_CAMERA = "142122070087"; // Reference camera for VGGT point cloud

      // DOM elements
      const container = document.getElementById("canvas-container");
      const loadingOverlay = document.getElementById("loading");
      const statusDot = document.getElementById("statusDot");
      const statusText = document.getElementById("statusText");
      const cameraCountEl = document.getElementById("cameraCount");
      const cameraListEl = document.getElementById("cameraList");
      const startBtn = document.getElementById("startBtn");
      
      // VGGT DOM elements
      const vggtEnabledCheckbox = document.getElementById("vggtEnabled");
      const vggtIntervalSelect = document.getElementById("vggtInterval");
      const vggtStatusEl = document.getElementById("vggtStatus");
      const vggtPointsEl = document.getElementById("vggtPoints");

      // Initialize Three.js scene
      function initScene() {
        // Scene
        scene = new THREE.Scene();
        scene.background = new THREE.Color(0xffffff);

        // Add fog for depth
        scene.fog = new THREE.Fog(0xffffff, 5, 20);

        // Camera
        camera = new THREE.PerspectiveCamera(
          60,
          window.innerWidth / window.innerHeight,
          0.1,
          100
        );
        // Position to see the multi-camera setup from above and side
        camera.position.set(1.5, 1.5, 2.5);
        camera.lookAt(0, 0, 0);

        // Renderer
        renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));
        container.appendChild(renderer.domElement);

        // Controls
        controls = new OrbitControls(camera, renderer.domElement);
        controls.enableDamping = true;
        controls.dampingFactor = 0.05;
        controls.minDistance = 0.5;
        controls.maxDistance = 15;
        controls.target.set(0, 0, 0);

        // Lighting
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
        scene.add(ambientLight);

        const directionalLight = new THREE.DirectionalLight(0xffffff, 0.8);
        directionalLight.position.set(5, 10, 5);
        scene.add(directionalLight);

        // Grid helper at global coordinate origin (0,0,0)
        const gridHelper = new THREE.GridHelper(3, 15, 0x888888, 0xcccccc);
        gridHelper.position.set(-1, -1.3, -2); // Positioned at world origin
        scene.add(gridHelper);

        // Handle resize
        window.addEventListener("resize", onWindowResize);
      }

      function onWindowResize() {
        camera.aspect = window.innerWidth / window.innerHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(window.innerWidth, window.innerHeight);
      }

      // Load calibration data
      async function loadCalibration() {
        try {
          const response = await fetch("/api/calibration");
          calibrationData = await response.json();
          console.log("Calibration loaded:", Object.keys(calibrationData));
          return calibrationData;
        } catch (error) {
          console.error("Failed to load calibration:", error);
          return {};
        }
      }

      // Load GLB scene - origin is at camera 141722079467, 138422075916
      async function loadGLBScene() {
        const loader = new GLTFLoader();
        const glbPath = "videos/VGGT_demo/calib_david_glbscene_5.2.glb";

        //  "videos/VGGT_demo/calib_glbscene_24.2_All_maskbFalse_maskwFalse_camTrue_skyFalse_predDepthmap_and_Camera_Branch.glb";
        //"videos/VGGT_demo/calib_glbscene_48.1_All_maskbFalse_maskwFalse_camTrue_skyFalse_predDepthmap_and_Camera_Branch.glb";
        //"videos/VGGT_demo/glbscene_97.3_All_maskbFalse_maskwFalse_camTrue_skyFalse_predDepthmap_and_Camera_Branch.glb";
        //"videos/VGGT_demo/glbscene_48.1_All_maskbFalse_maskwFalse_camTrue_skyFalse_predDepthmap_and_Camera_Branch.glb";
        // "/videos/VGGT_demo/glbscene_50_All_maskbFalse_maskwFalse_camTrue_skyFalse_predDepthmap_and_Camera_Branch.glb";
        const glbOriginCameraId = "135122071615";

        return new Promise((resolve, reject) => {
          loader.load(
            glbPath,
            (gltf) => {
              console.log("GLB scene loaded successfully");
              const model = gltf.scene;

              // The GLB's origin is at camera 141722079467's viewpoint
              // We need to position the GLB at that camera's world position
              const calibration = calibrationData[glbOriginCameraId];
              if (calibration && calibration.extrinsics) {
                const ext = calibration.extrinsics;

                // Extract translation and rotation from camera extrinsics
                const t = [ext[0][3], ext[1][3], ext[2][3]];
                const R = [
                  [ext[0][0], ext[0][1], ext[0][2]],
                  [ext[1][0], ext[1][1], ext[1][2]],
                  [ext[2][0], ext[2][1], ext[2][2]],
                ];

                // Convert from OpenCV to Three.js coordinate system
                const position = new THREE.Vector3(t[0], -t[1], -t[2]);

                // Build rotation matrix in Three.js coordinates
                const rotMatrix = new THREE.Matrix4();
                rotMatrix.set(
                  R[0][0],
                  -R[0][1],
                  -R[0][2],
                  0,
                  -R[1][0],
                  R[1][1],
                  R[1][2],
                  0,
                  -R[2][0],
                  R[2][1],
                  R[2][2],
                  0,
                  0,
                  0,
                  0,
                  1
                );

                const quaternion = new THREE.Quaternion();
                quaternion.setFromRotationMatrix(rotMatrix);

                // Rotate 180 degrees around Y axis to face correct direction
                const flip180 = new THREE.Quaternion();
                flip180.setFromAxisAngle(new THREE.Vector3(0, 1, 0), Math.PI);
                quaternion.multiply(flip180);

                // Apply transformation to model
                model.position.copy(position);
                model.quaternion.copy(quaternion);

                // Scale if needed (uncomment if GLB is in mm instead of meters)
                model.scale.set(1, 1, 1);

                console.log(
                  `GLB model positioned at camera ${glbOriginCameraId}:`,
                  position.toArray().map((v) => v.toFixed(3))
                );
              } else {
                console.warn("Calibration not found for GLB origin camera");
                model.position.set(0, 0, 0);
              }

              scene.add(model);
              console.log("GLB model added to scene");
              resolve(model);
            },
            (progress) => {
              const percent = (progress.loaded / progress.total) * 100;
              console.log(`Loading GLB: ${percent.toFixed(1)}%`);
            },
            (error) => {
              console.error("Error loading GLB:", error);
              reject(error);
            }
          );
        });
      }

      // Load DOPE objects configuration from server
      async function loadDopeObjectsConfig() {
        try {
          const response = await fetch("/api/objects/config");
          objectConfigs = await response.json();
          console.log("DOPE objects config loaded:", Object.keys(objectConfigs));
          return objectConfigs;
        } catch (error) {
          console.error("Failed to load DOPE objects config:", error);
          return {};
        }
      }

      // Load OBJ model with MTL material for a specific object
      async function loadOBJModel(objectName, objPath) {
        // Convert the obj_path from server format to URL path
        // obj_path: "data/scanned_objects/e-screw-driver/eScrewDriver.obj"
        // URL: "videos/scanned_objects/e-screw-driver/eScrewDriver.obj"
        const urlPath = objPath.replace("data/", "videos/");
        const lastSlashIndex = urlPath.lastIndexOf("/");
        const basePath = urlPath.substring(0, lastSlashIndex + 1);
        const objFileName = urlPath.substring(lastSlashIndex + 1);
        const mtlFileName = objFileName.replace(".obj", ".mtl");

        console.log(`[${objectName}] Starting OBJ model load...`);
        console.log(`[${objectName}] Base path:`, basePath);
        console.log(`[${objectName}] OBJ file:`, objFileName);
        console.log(`[${objectName}] MTL file:`, mtlFileName);

        return new Promise((resolve, reject) => {
          const mtlLoader = new MTLLoader();
          mtlLoader.setPath(basePath);

          mtlLoader.load(
            mtlFileName,
            (materials) => {
              materials.preload();
              console.log(`[${objectName}] MTL materials loaded`);

              const objLoader = new OBJLoader();
              objLoader.setMaterials(materials);

              objLoader.load(
                basePath + objFileName,
                (object) => {
                  console.log(`[${objectName}] OBJ model loaded successfully`);

                  // Apply scale
                  object.scale.set(1, 1, 1);

                  // Calculate bounding box to find centroid offset
                  const boundingBox = new THREE.Box3().setFromObject(object);
                  const center = new THREE.Vector3();
                  boundingBox.getCenter(center);

                  // Store the centroid offset (in model's local coordinates)
                  objectCentroids[objectName] = center.clone();
                  console.log(
                    `[${objectName}] Model centroid offset:`,
                    center.toArray().map((v) => v.toFixed(4))
                  );

                  // Initially hidden until we get a pose
                  object.visible = false;
                  object.position.set(0, 0, 0);
                  object.userData.objectName = objectName;

                  // Enable shadows if needed
                  object.traverse((child) => {
                    if (child instanceof THREE.Mesh) {
                      child.castShadow = true;
                      child.receiveShadow = true;
                    }
                  });

                  scene.add(object);

                  // Store reference for pose updates
                  objectModels[objectName] = object;

                  console.log(
                    `[${objectName}] OBJ model added to scene (hidden until DOPE detection)`
                  );
                  resolve(object);
                },
                (progress) => {
                  if (progress.total > 0) {
                    const percent = (progress.loaded / progress.total) * 100;
                    console.log(`[${objectName}] Loading OBJ: ${percent.toFixed(1)}%`);
                  }
                },
                (error) => {
                  console.error(`[${objectName}] Error loading OBJ file:`, basePath + objFileName);
                  console.error(`[${objectName}] OBJ Error details:`, error);
                  reject(error);
                }
              );
            },
            (progress) => {
              // MTL loading progress
            },
            (error) => {
              console.error(`[${objectName}] Error loading MTL file:`, basePath + mtlFileName);
              console.error(`[${objectName}] MTL Error details:`, error);
              reject(error);
            }
          );
        });
      }

      // Load all configured DOPE objects
      async function loadAllDopeObjects() {
        await loadDopeObjectsConfig();
        
        const loadPromises = [];
        for (const [objectName, config] of Object.entries(objectConfigs)) {
          if (config.obj_path) {
            loadPromises.push(
              loadOBJModel(objectName, config.obj_path).catch((error) => {
                console.warn(`[${objectName}] Failed to load model, continuing:`, error);
                return null;
              })
            );
          }
        }
        
        await Promise.all(loadPromises);
        console.log(`Loaded ${Object.keys(objectModels).length} object models`);
      }

      // Fetch and update poses for all detected objects
      async function updateObjectPoses() {
        if (Object.keys(objectModels).length === 0) return;

        try {
          const response = await fetch("/api/objects/poses");
          const posesData = await response.json();

          let detectedCount = 0;
          let totalObjects = Object.keys(objectModels).length;
          let firstDetectedPos = null;

          // Update each object
          for (const [objectName, poseData] of Object.entries(posesData)) {
            const model = objectModels[objectName];
            const centroid = objectCentroids[objectName];
            const config = objectConfigs[objectName];
            
            if (!model || !centroid || !config) continue;

            if (poseData && poseData.detected) {
              // Get the camera calibration for this object's assigned camera
              const cameraId = poseData.camera_id || config.camera_id;
              const camCalib = calibrationData[cameraId];
              if (!camCalib || !camCalib.extrinsics) {
                console.warn(`No calibration for camera ${cameraId} (object: ${objectName})`);
                continue;
              }

              detectedCount++;

              // Extract camera extrinsics (camera to world transform)
              const ext = camCalib.extrinsics;
              const camR = new THREE.Matrix4();
              camR.set(
                ext[0][0],
                -ext[0][1],
                -ext[0][2],
                0,
                -ext[1][0],
                ext[1][1],
                ext[1][2],
                0,
                -ext[2][0],
                ext[2][1],
                ext[2][2],
                0,
                0,
                0,
                0,
                1
              );
              const camPos = new THREE.Vector3(ext[0][3], -ext[1][3], -ext[2][3]);
              const camQuat = new THREE.Quaternion().setFromRotationMatrix(camR);

              // Object pose from DOPE (in camera coordinates)
              const objLocation = poseData.location; // [x, y, z] in meters
              const objQuaternion = poseData.quaternion; // [x, y, z, w]

              // Convert object position from camera to world coordinates
              // In OpenCV camera: X-right, Y-down, Z-forward
              // In Three.js: X-right, Y-up, Z-backward
              const objLocalPos = new THREE.Vector3(
                objLocation[0],
                -objLocation[1], // Flip Y
                -objLocation[2] // Flip Z
              );

              // Transform to world coordinates
              objLocalPos.applyQuaternion(camQuat);
              const worldPos = objLocalPos.add(camPos);

              // Store first detected position for UI
              if (!firstDetectedPos) {
                firstDetectedPos = worldPos.clone();
              }

              // Convert object rotation from OpenCV to Three.js
              // Quaternion from DOPE is in camera frame
              const objQuat = new THREE.Quaternion(
                objQuaternion[0],
                -objQuaternion[1], // Flip Y
                -objQuaternion[2], // Flip Z
                objQuaternion[3]
              );

              // Combine with camera rotation
              const worldQuat = camQuat.clone().multiply(objQuat);

              // Flip the object 180 degrees around Y axis (in XZ plane)
              const flipY = new THREE.Quaternion();
              flipY.setFromAxisAngle(new THREE.Vector3(0, 0, 1), Math.PI);
              const flipZ = new THREE.Quaternion();
              flipZ.setFromAxisAngle(new THREE.Vector3(0, 1, 0), Math.PI);
              const finalQuat = worldQuat.clone().multiply(flipY).multiply(flipZ);

              // Apply pose to model's centroid (not origin)
              // Rotate the centroid offset by the final rotation (including flip)
              const rotatedCentroid = centroid.clone().applyQuaternion(finalQuat);

              // Offset position so centroid is at the detected pose
              const modelPos = worldPos.clone().sub(rotatedCentroid);
              //const modelPos = worldPos.clone();
              // Apply to model - always visible once detected
              model.position.copy(modelPos);
              model.quaternion.copy(finalQuat);
              model.visible = true;
            }
          }

          // Update status indicators
          if (detectedCount > 0) {
            statusDot.classList.add("active");
            statusText.textContent = `${detectedCount}/${totalObjects} Objects`;
          } else {
            if (!streaming) {
              statusText.textContent = "Ready";
            } else {
              statusText.textContent = "Streaming";
            }
          }
        } catch (error) {
          // Silently handle errors to avoid console spam
        }
      }

      // Start polling for object pose updates
      function startPosePolling() {
        if (posePollingInterval) return;
        posePollingInterval = setInterval(updateObjectPoses, 100); // Poll every 100ms
        console.log("[Pose] Started polling for object poses");
      }

      // Stop polling for object pose updates
      function stopPosePolling() {
        if (posePollingInterval) {
          clearInterval(posePollingInterval);
          posePollingInterval = null;
          console.log("[Pose] Stopped polling for object poses");
        }
        // Keep models visible at last known position (don't hide them)
      }

      // ==========================================================
      // VGGT Point Cloud Functions
      // ==========================================================

      // Initialize VGGT point cloud visualization
      function initVGGTPointCloud() {
        // Create a group to hold the point cloud with transformation
        vggtPointCloudGroup = new THREE.Group();
        
        // Create initial empty point cloud
        const geometry = new THREE.BufferGeometry();
        geometry.setAttribute('position', new THREE.Float32BufferAttribute([], 3));
        geometry.setAttribute('color', new THREE.Float32BufferAttribute([], 3));
        
        const material = new THREE.PointsMaterial({
          size: 0.005,
          vertexColors: true,
          sizeAttenuation: true
        });
        
        vggtPointCloud = new THREE.Points(geometry, material);
        vggtPointCloudGroup.add(vggtPointCloud);
        vggtPointCloudGroup.visible = false;
        
        // Position the point cloud group relative to reference camera (135122071615)
        // Same transformation as the GLB scene
        const calibration = calibrationData[VGGT_REFERENCE_CAMERA];
        if (calibration && calibration.extrinsics) {
          const ext = calibration.extrinsics;
          
          // Extract translation and rotation from camera extrinsics
          const t = [ext[0][3], ext[1][3], ext[2][3]];
          const R = [
            [ext[0][0], ext[0][1], ext[0][2]],
            [ext[1][0], ext[1][1], ext[1][2]],
            [ext[2][0], ext[2][1], ext[2][2]],
          ];
          
          // Convert from OpenCV to Three.js coordinate system
          const position = new THREE.Vector3(t[0], -t[1], -t[2]);
          
          // Build rotation matrix in Three.js coordinates
          const rotMatrix = new THREE.Matrix4();
          rotMatrix.set(
            R[0][0], -R[0][1], -R[0][2], 0,
            -R[1][0], R[1][1], R[1][2], 0,
            -R[2][0], R[2][1], R[2][2], 0,
            0, 0, 0, 1
          );
          
          const quaternion = new THREE.Quaternion();
          quaternion.setFromRotationMatrix(rotMatrix);
          
          // Rotate 180 degrees around Y axis to face correct direction
          const flip180 = new THREE.Quaternion();
          flip180.setFromAxisAngle(new THREE.Vector3(0, 1, 0), Math.PI);
          quaternion.multiply(flip180);
          
          // Apply transformation to the group
          vggtPointCloudGroup.position.copy(position);
          vggtPointCloudGroup.quaternion.copy(quaternion);
          
          console.log(`[VGGT] Point cloud positioned at camera ${VGGT_REFERENCE_CAMERA}:`, 
            position.toArray().map(v => v.toFixed(3)));
        } else {
          console.warn("[VGGT] No calibration found for reference camera, using origin");
        }
        
        scene.add(vggtPointCloudGroup);
        console.log("[VGGT] Point cloud initialized");
      }

      // Update VGGT point cloud from server data
      async function updateVGGTPointCloud() {
        if (!vggtEnabled) return;
        
        try {
          const response = await fetch("/api/vggt/pointcloud");
          const data = await response.json();
          
          if (data.success && data.num_points > 0) {
            // Update geometry with new points
            const positions = new Float32Array(data.points.flat());
            const colors = new Float32Array(data.colors.flat().map(c => c / 255)); // Normalize to 0-1
            
            vggtPointCloud.geometry.setAttribute('position', 
              new THREE.Float32BufferAttribute(positions, 3));
            vggtPointCloud.geometry.setAttribute('color', 
              new THREE.Float32BufferAttribute(colors, 3));
            vggtPointCloud.geometry.computeBoundingSphere();
            vggtPointCloudGroup.visible = true;
            
            // Update UI
            vggtPointsEl.textContent = data.num_points.toLocaleString();
            vggtStatusEl.textContent = `${data.num_points.toLocaleString()} pts`;
            vggtStatusEl.style.color = '#00ff88';
            vggtLastPointCount = data.num_points;
            
          } else if (vggtLastPointCount === 0) {
            vggtStatusEl.textContent = 'Waiting...';
            vggtStatusEl.style.color = '#ffaa00';
          }
        } catch (error) {
          // Silently handle errors
        }
      }

      // Start polling for VGGT point cloud
      function startVGGTPolling() {
        if (vggtPollingInterval) return;
        vggtPollingInterval = setInterval(updateVGGTPointCloud, 500); // Poll every 500ms
        console.log("[VGGT] Started polling for point cloud");
      }

      // Stop polling for VGGT point cloud
      function stopVGGTPolling() {
        if (vggtPollingInterval) {
          clearInterval(vggtPollingInterval);
          vggtPollingInterval = null;
          console.log("[VGGT] Stopped polling for point cloud");
        }
      }

      // Toggle VGGT enabled state
      async function setVGGTEnabled(enabled) {
        try {
          const response = await fetch("/api/vggt/enable", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ enabled })
          });
          const result = await response.json();
          
          if (result.success) {
            vggtEnabled = enabled;
            vggtEnabledCheckbox.checked = enabled;
            
            if (enabled) {
              vggtStatusEl.textContent = 'Starting...';
              vggtStatusEl.style.color = '#ffaa00';
              if (streaming) {
                startVGGTPolling();
              }
            } else {
              vggtStatusEl.textContent = 'Off';
              vggtStatusEl.style.color = '#888';
              stopVGGTPolling();
              if (vggtPointCloudGroup) {
                vggtPointCloudGroup.visible = false;
              }
            }
            
            console.log(`[VGGT] ${enabled ? 'Enabled' : 'Disabled'}`);
          }
        } catch (error) {
          console.error("[VGGT] Failed to toggle:", error);
        }
      }

      // Set VGGT inference interval
      async function setVGGTInterval(interval) {
        try {
          const response = await fetch("/api/vggt/interval", {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({ interval: parseInt(interval) })
          });
          const result = await response.json();
          
          if (result.success) {
            console.log(`[VGGT] Interval set to ${interval} frames`);
          }
        } catch (error) {
          console.error("[VGGT] Failed to set interval:", error);
        }
      }

      // Load VGGT status from server
      async function loadVGGTStatus() {
        try {
          const response = await fetch("/api/vggt/status");
          const status = await response.json();
          
          vggtEnabledCheckbox.checked = status.enabled;
          vggtIntervalSelect.value = status.inference_interval.toString();
          vggtEnabled = status.enabled;
          
          if (!status.loaded) {
            vggtStatusEl.textContent = 'Not loaded';
            vggtStatusEl.style.color = '#ff5555';
            vggtEnabledCheckbox.disabled = true;
          } else if (status.enabled) {
            vggtStatusEl.textContent = 'Active';
            vggtStatusEl.style.color = '#00ff88';
          } else {
            vggtStatusEl.textContent = 'Off';
            vggtStatusEl.style.color = '#888';
          }
          
          console.log("[VGGT] Status loaded:", status);
        } catch (error) {
          console.error("[VGGT] Failed to load status:", error);
        }
      }

      // Load available cameras
      async function loadCameras() {
        try {
          const response = await fetch("/api/cameras");
          return await response.json();
        } catch (error) {
          console.error("Failed to load cameras:", error);
          return [];
        }
      }

      // Create camera plane with video texture
      function createCameraPlane(cameraId, colorIndex) {
        const planeWidth = 0.3; // 30cm in scene units (meters)
        const planeHeight = 0.17; // Maintain 16:9 aspect ratio

        // Create plane geometry
        const geometry = new THREE.PlaneGeometry(planeWidth, planeHeight);

        // Create canvas for texture
        const canvas = document.createElement("canvas");
        canvas.width = 320;
        canvas.height = 180;
        const ctx = canvas.getContext("2d");

        // Initial placeholder
        ctx.fillStyle = "#1a1a25";
        ctx.fillRect(0, 0, canvas.width, canvas.height);
        ctx.fillStyle = "#555566";
        ctx.font = "14px JetBrains Mono";
        ctx.textAlign = "center";
        ctx.fillText(`Camera ${cameraId}`, canvas.width / 2, canvas.height / 2);

        // Create texture
        const texture = new THREE.CanvasTexture(canvas);
        texture.minFilter = THREE.LinearFilter;
        texture.magFilter = THREE.LinearFilter;

        // Material
        const material = new THREE.MeshBasicMaterial({
          map: texture,
          side: THREE.DoubleSide,
        });

        // Mesh
        const plane = new THREE.Mesh(geometry, material);
        plane.userData = {
          cameraId: cameraId,
          canvas: canvas,
          ctx: ctx,
          texture: texture,
        };

        // Add border frame
        const borderGeometry = new THREE.EdgesGeometry(geometry);
        const borderMaterial = new THREE.LineBasicMaterial({
          color: CAMERA_COLORS[colorIndex % CAMERA_COLORS.length],
        });
        const border = new THREE.LineSegments(borderGeometry, borderMaterial);
        plane.add(border);

        return plane;
      }

      // Create camera frustum visualization
      function createCameraFrustum(cameraId, colorIndex) {
        const color = CAMERA_COLORS[colorIndex % CAMERA_COLORS.length];
        const frustumLength = 0.15;
        const frustumWidth = 0.08;
        const frustumHeight = 0.045;

        // Create frustum lines - pointing along -Z (Three.js camera convention)
        const points = [
          // Origin
          new THREE.Vector3(0, 0, 0),
          // Four corners of far plane (negative Z direction)
          new THREE.Vector3(-frustumWidth, -frustumHeight, -frustumLength),
          new THREE.Vector3(frustumWidth, -frustumHeight, -frustumLength),
          new THREE.Vector3(frustumWidth, frustumHeight, -frustumLength),
          new THREE.Vector3(-frustumWidth, frustumHeight, -frustumLength),
        ];

        const lineGeometry = new THREE.BufferGeometry();
        const linePositions = [
          // Lines from origin to corners
          0,
          0,
          0,
          points[1].x,
          points[1].y,
          points[1].z,
          0,
          0,
          0,
          points[2].x,
          points[2].y,
          points[2].z,
          0,
          0,
          0,
          points[3].x,
          points[3].y,
          points[3].z,
          0,
          0,
          0,
          points[4].x,
          points[4].y,
          points[4].z,
          // Rectangle at far plane
          points[1].x,
          points[1].y,
          points[1].z,
          points[2].x,
          points[2].y,
          points[2].z,
          points[2].x,
          points[2].y,
          points[2].z,
          points[3].x,
          points[3].y,
          points[3].z,
          points[3].x,
          points[3].y,
          points[3].z,
          points[4].x,
          points[4].y,
          points[4].z,
          points[4].x,
          points[4].y,
          points[4].z,
          points[1].x,
          points[1].y,
          points[1].z,
        ];

        lineGeometry.setAttribute(
          "position",
          new THREE.Float32BufferAttribute(linePositions, 3)
        );
        const lineMaterial = new THREE.LineBasicMaterial({
          color: color,
          opacity: 0.7,
          transparent: true,
        });
        const frustum = new THREE.LineSegments(lineGeometry, lineMaterial);

        // Camera body (small box)
        const bodyGeometry = new THREE.BoxGeometry(0.04, 0.03, 0.02);
        const bodyMaterial = new THREE.MeshBasicMaterial({ color: color });
        const body = new THREE.Mesh(bodyGeometry, bodyMaterial);
        body.position.z = 0.01;
        frustum.add(body);

        return frustum;
      }

      // Position camera in scene based on extrinsics
      function positionCamera(cameraId, plane, frustum) {
        const calibration = calibrationData[cameraId];
        if (!calibration || !calibration.extrinsics) {
          console.warn(`No extrinsics for camera ${cameraId}`);
          return;
        }

        const ext = calibration.extrinsics;

        // Extract rotation matrix R and translation vector t from 4x4 matrix
        // The extrinsics are camera-to-world transforms
        const R = [
          [ext[0][0], ext[0][1], ext[0][2]],
          [ext[1][0], ext[1][1], ext[1][2]],
          [ext[2][0], ext[2][1], ext[2][2]],
        ];
        const t = [ext[0][3], ext[1][3], ext[2][3]];

        // Convert from OpenCV to Three.js coordinate system:
        // OpenCV: X-right, Y-down, Z-forward
        // Three.js: X-right, Y-up, Z-backward
        const position = new THREE.Vector3(
          t[0],
          -t[1], // Flip Y
          -t[2] // Flip Z
        );

        // Use rotation from extrinsics
        // Convert rotation: R' = F * R * F where F = diag(1,-1,-1)
        const rotMatrix = new THREE.Matrix4();
        rotMatrix.set(
          R[0][0],
          -R[0][1],
          -R[0][2],
          0,
          -R[1][0],
          R[1][1],
          R[1][2],
          0,
          -R[2][0],
          R[2][1],
          R[2][2],
          0,
          0,
          0,
          0,
          1
        );
        const quaternion = new THREE.Quaternion();
        quaternion.setFromRotationMatrix(rotMatrix);

        // Position the frustum at camera location
        frustum.position.copy(position);
        frustum.quaternion.copy(quaternion);

        // Position plane in front of camera (along -Z in camera local space)
        const planeOffset = 0.15;
        const lookDir = new THREE.Vector3(0, 0, -1);
        lookDir.applyQuaternion(quaternion);

        plane.position.copy(position).addScaledVector(lookDir, planeOffset);
        plane.quaternion.copy(quaternion);

        console.log(
          `Camera ${cameraId} (#${calibration.number}) pos:`,
          position.toArray().map((v) => v.toFixed(3))
        );
      }

      // Create UI for camera list
      function createCameraListUI(cameras) {
        cameraListEl.innerHTML = "";

        cameras.forEach((camId, index) => {
          const item = document.createElement("div");
          item.className = "camera-item";
          item.dataset.cameraId = camId;

          const color = CAMERA_COLORS[index % CAMERA_COLORS.length];
          const calibration = calibrationData[camId];
          const camNumber = calibration ? calibration.number : index + 1;
          const isMaster = calibration && calibration.master;
          const hasYolo = calibration && calibration.yolo_enabled;

          let badges = "";
          if (isMaster) badges += " (Master)";
          if (hasYolo)
            badges +=
              ' <span style="color:#00ff88;font-weight:600;">YOLO</span>';

          item.innerHTML = `
            <div class="camera-color" style="background: #${color
              .toString(16)
              .padStart(6, "0")}"></div>
            <div class="camera-info">
              <div class="camera-id">${camId}</div>
              <div class="camera-number">Camera #${camNumber}${badges}</div>
            </div>
          `;

          item.addEventListener("click", () => focusOnCamera(camId));
          cameraListEl.appendChild(item);
        });
      }

      // Focus view on specific camera
      function focusOnCamera(cameraId) {
        const plane = cameraPlanes[cameraId];
        if (!plane) return;

        // Animate camera to look at plane
        const targetPos = plane.position.clone();

        // Calculate good viewing position
        const direction = new THREE.Vector3(0, 0, 1);
        direction.applyQuaternion(plane.quaternion);

        const viewPos = targetPos.clone().add(direction.multiplyScalar(1.5));
        viewPos.y += 0.5;

        // Update controls target
        controls.target.copy(targetPos);
        camera.position.copy(viewPos);

        // Update selected state in UI
        document.querySelectorAll(".camera-item").forEach((item) => {
          item.classList.toggle("selected", item.dataset.cameraId === cameraId);
        });
      }

      // Start streaming frames
      async function startStreaming() {
        if (streaming) return;

        // Tell the server to start processing frames
        try {
          const response = await fetch("/api/stream/start", { method: "POST" });
          const result = await response.json();
          console.log("[Stream] Server started:", result);
        } catch (error) {
          console.error("[Stream] Failed to start server streaming:", error);
          return;
        }

        streaming = true;

        // Start pose polling for real-time tool position updates
        startPosePolling();
        
        // Start VGGT polling if enabled
        if (vggtEnabled) {
          startVGGTPolling();
        }

        Object.keys(cameraPlanes).forEach((cameraId) => {
          const plane = cameraPlanes[cameraId];
          const { canvas, ctx, texture } = plane.userData;

          // Create image element for loading frames
          const img = new Image();
          img.crossOrigin = "anonymous";

          const updateFrame = () => {
            if (!streaming) return;

            // Add timestamp to avoid caching
            img.src = `/api/frame/${cameraId}?t=${Date.now()}`;
          };

          img.onload = () => {
            ctx.drawImage(img, 0, 0, canvas.width, canvas.height);
            texture.needsUpdate = true;

            if (streaming) {
              // Request next frame
              setTimeout(updateFrame, 66); // ~15 FPS
            }
          };

          img.onerror = () => {
            if (streaming) {
              setTimeout(updateFrame, 500);
            }
          };

          // Start loading frames
          updateFrame();
        });

        startBtn.textContent = "Stop Streams";
        statusDot.classList.add("active");
        statusText.textContent = "Streaming";
      }

      // Stop streaming
      async function stopStreaming() {
        streaming = false;

        // Stop pose polling
        stopPosePolling();
        
        // Stop VGGT polling
        stopVGGTPolling();

        // Tell the server to stop processing frames
        try {
          const response = await fetch("/api/stream/stop", { method: "POST" });
          const result = await response.json();
          console.log("[Stream] Server stopped:", result);
        } catch (error) {
          console.error("[Stream] Failed to stop server streaming:", error);
        }

        startBtn.textContent = "Start Streams";
        statusDot.classList.remove("active");
        statusText.textContent = "Paused";
      }

      // Animation loop
      function animate() {
        requestAnimationFrame(animate);

        controls.update();
        renderer.render(scene, camera);
      }

      // Initialize everything
      async function init() {
        // Initialize Three.js
        initScene();

        // Load calibration first (needed for GLB positioning)
        await loadCalibration();
        availableCameras = await loadCameras();

        console.log("Available cameras:", availableCameras);

        // Load GLB scene model (after calibration so we can position it)
        try {
          await loadGLBScene();
        } catch (error) {
          console.error("Failed to load GLB scene, continuing anyway:", error);
        }

        // Load all DOPE object models (tool, case, etc.)
        try {
          await loadAllDopeObjects();
        } catch (error) {
          console.error("Failed to load DOPE object models, continuing anyway:", error);
        }
        
        // Initialize VGGT point cloud visualization
        initVGGTPointCloud();
        
        // Load VGGT status from server
        await loadVGGTStatus();

        // Create camera planes and frustums
        availableCameras.forEach((cameraId, index) => {
          // Create plane with video texture
          const plane = createCameraPlane(cameraId, index);
          cameraPlanes[cameraId] = plane;
          scene.add(plane);

          // Create frustum visualization
          const frustum = createCameraFrustum(cameraId, index);
          cameraFrustums[cameraId] = frustum;
          scene.add(frustum);

          // Position based on extrinsics
          positionCamera(cameraId, plane, frustum, false);
        });

        // Update UI
        cameraCountEl.textContent = availableCameras.length.toString();
        createCameraListUI(availableCameras);

        // Event listeners
        startBtn.addEventListener("click", () => {
          if (streaming) {
            stopStreaming();
          } else {
            startStreaming();
          }
        });
        
        // VGGT event listeners
        vggtEnabledCheckbox.addEventListener("change", (e) => {
          setVGGTEnabled(e.target.checked);
        });
        
        vggtIntervalSelect.addEventListener("change", (e) => {
          setVGGTInterval(e.target.value);
        });

        // Hide loading overlay
        loadingOverlay.classList.add("hidden");
        statusText.textContent = "Ready";

        // Start animation
        animate();
      }

      // Start
      init().catch(console.error);
    </script>
  </body>
</html>
